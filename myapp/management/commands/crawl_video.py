import re
import requests
import logging
from django.core.management.base import BaseCommand
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urlencode, urljoin
from dateutil.parser import parse
from myapp.models import Video
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import xml.etree.ElementTree as ET
from datetime import datetime

logger = logging.getLogger(__name__)


class CmaCatalogCrawler:
    def __init__(self, url, command):
        self.base_url = "https://www.cma.org.cn"
        self.command = command
        self.session = requests.Session()

        # é…ç½®è¯·æ±‚å¤´å’Œé‡è¯•ç­–ç•¥
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': url,
            'X-Requested-With': 'XMLHttpRequest',
            'Origin': self.base_url
        }

        retries = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[500, 502, 503, 504]
        )
        self.session.mount('https://', HTTPAdapter(max_retries=retries))

    def fetch_page(self, page_num):
        """è·å–åˆ†é¡µæ•°æ®ï¼ˆXMLæ ¼å¼ï¼‰"""
        try:
            response = self.session.post(
                'https://www.cma.org.cn/module/web/jpage/dataproxy.jsp',
                headers={
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Origin': 'https://www.cma.org.cn',
                    'Referer': self.base_url + '/col/col982/index.html',
                    **self.headers
                },
                data={
                    'col': '1',
                    'webid': '1',
                    'path': '/',
                    'columnid': '982',
                    'sourceContentType': '1',
                    'unitid': '325',
                    'webname': '%E4%B8%AD%E5%8D%8E%E5%8C%BB%E5%AD%A6%E4%BC%9A',  # URLç¼–ç åçš„ä¸­æ–‡
                    'permissiontype': '0',
                    'page': page_num,
                    'pageSize': '20',
                    'uid': '325'  # æ–°å¢å¿…è¦å‚æ•°
                },
                timeout=15
            )
            response.raise_for_status()

            # ä¿®å¤XMLè§£æ
            root = ET.fromstring(response.text)
            return {
                'total': int(root.find('totalrecord').text),
                'pages': int(root.find('totalpage').text),
                'html': ''.join([r.text for r in root.findall('recordset/record')])
            }
        except Exception as e:
            self.command.stdout.write(
                self.command.style.WARNING(f"ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥: {str(e)}")
            )
            return None

    def parse_links(self, html_content):
        """è§£æHTMLå†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = []

        for li in soup.find_all('li'):
            a_tag = li.find('a', target='_blank')
            if not a_tag or not a_tag.get('href'):
                continue

            full_url = urljoin(self.base_url, a_tag['href'])

            # æ—¥æœŸè§£æå¢å¼º
            date_span = li.find('span', class_='riq')
            pub_date = None
            if date_span and date_span.text.strip():
                try:
                    pub_date = datetime.strptime(date_span.text.strip(), "%Yå¹´%mæœˆ%dæ—¥")
                except ValueError:
                    try:
                        pub_date = parse(date_span.text.strip())
                    except:
                        pass

            links.append({
                'url': full_url,
                'published_at': pub_date
            })

            self.command.stdout.write(
                self.command.style.NOTICE(f"å‘ç°é“¾æ¥: {full_url}" +
                                          (f" æ—¥æœŸ: {pub_date}" if pub_date else ""))
            )
        return links

    def crawl(self):
        """æ‰§è¡Œçˆ¬å–æµç¨‹"""
        all_links = []
        try:
            # è·å–ç¬¬ä¸€é¡µæ•°æ®
            first_page = self.fetch_page(1)
            if not first_page:
                return []

            total_pages = first_page['pages']
            self.command.stdout.write(
                self.command.style.SUCCESS(f"æ€»é¡µæ•°: {total_pages} æ€»è®°å½•: {first_page['total']}")
            )

            # å¤„ç†ç¬¬ä¸€é¡µ
            page_links = self.parse_links(first_page['html'])
            all_links.extend(page_links)
            self.command.stdout.write(
                self.command.style.SUCCESS(f"ç¬¬ 1/{total_pages} é¡µ æ‰¾åˆ° {len(page_links)} æ¡è®°å½•")
            )

            # éå†åç»­é¡µ
            for page in range(2, total_pages + 1):
                page_data = self.fetch_page(page)
                if not page_data:
                    continue

                page_links = self.parse_links(page_data['html'])
                all_links.extend(page_links)
                self.command.stdout.write(
                    self.command.style.SUCCESS(f"ç¬¬ {page}/{total_pages} é¡µ æ‰¾åˆ° {len(page_links)} æ¡è®°å½•")
                )

        except Exception as e:
            self.command.stdout.write(
                self.command.style.ERROR(f"ç›®å½•çˆ¬å–å¤±è´¥: {str(e)}")
            )
            logger.exception("Crawl error")

        return all_links


class CmaVideoCrawler:
    """è§†é¢‘è¯¦æƒ…é¡µå¤„ç†å™¨"""

    def __init__(self, url, list_published_at=None):
        self.url = url
        self.list_published_at = list_published_at
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.cma.org.cn/col/col982/index.html'
        }

    def parse_video(self):
        """è§£æè§†é¢‘è¯¦æƒ…é¡µ"""
        try:
            response = self.session.get(self.url, headers=self.headers, timeout=15)
            response.raise_for_status()
        except requests.RequestException as e:
            raise ValueError(f"è¯·æ±‚å¤±è´¥: {str(e)}")

        soup = BeautifulSoup(response.text, 'html.parser')

        # æ ‡é¢˜è§£æ
        title = soup.select_one('td.title').text.strip() if soup.select_one('td.title') else 'æ— æ ‡é¢˜'

        # è§£æ <video> æ ‡ç­¾
        video_url = None
        video_tag = soup.find('video')
        if video_tag:
            src = video_tag.get('src')
            if src:
                video_url = urljoin(self.url, src)  # è§£å†³ç›¸å¯¹è·¯å¾„é—®é¢˜

        # å¦‚æœ <video> æ²¡æœ‰ srcï¼Œåˆ™å°è¯•ä» JavaScript é‡Œæ‰¾
        if not video_url:
            script_tags = soup.find_all('script', text=True)
            for script in script_tags:
                match = re.search(r'flashvars=\{[^}]*f:"(.*?)"', script.text)
                if match:
                    video_url = urljoin(self.url, match.group(1))
                    break

        # è§£æå‘å¸ƒæ—¥æœŸ
        published_at = self.list_published_at
        if not published_at:
            date_text = soup.find(text=lambda t: 'å‘å¸ƒæ—¥æœŸ' in str(t))
            if date_text:
                try:
                    published_at = parse(date_text.split('ï¼š')[1].strip())
                except:
                    published_at = None

        return {
            'title': title,
            'video_url': video_url,
            'published_at': published_at,
            'source_name': "ä¸­ååŒ»å­¦ä¼šç§‘å­¦æ™®åŠéƒ¨"
        }

    def save_to_db(self, data):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        if not data['video_url']:
            raise ValueError("æ— æ•ˆçš„è§†é¢‘åœ°å€")

        # å»é‡æ£€æŸ¥
        if Video.objects.filter(video_url=data['video_url']).exists():
            return {'status': 'exists', 'message': 'è§†é¢‘å·²å­˜åœ¨'}

        try:
            video = Video.objects.create(
                title=data['title'],
                video_url=data['video_url'],
                published_at=data['published_at'],
                source_name=data['source_name']
            )
            return {'status': 'success', 'video': video}
        except Exception as e:
            raise ValueError(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)}")

    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        try:
            video_data = self.parse_video()
            result = self.save_to_db(video_data)
            if result['status'] == 'success':
                return {
                    'status': 'success',
                    'title': result['video'].title,
                    'video_id': result['video'].id
                }
            return result
        except Exception as e:
            return {'status': 'error', 'message': str(e)}


class Command(BaseCommand):
    help = "çˆ¬å–ä¸­ååŒ»å­¦ä¼šç§‘æ™®è§†é¢‘"

    def add_arguments(self, parser):
        parser.add_argument('url', type=str, help='ç›®æ ‡URLï¼ˆç›®å½•é¡µæˆ–è§†é¢‘é¡µï¼‰')
        parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
        parser.add_argument('--max-page', type=int, default=0, help='æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰')

    def handle_catalog(self, url, options):
        """å¤„ç†ç›®å½•é¡µçˆ¬å–"""
        self.stdout.write(self.style.SUCCESS("ğŸš€ å¯åŠ¨ç›®å½•çˆ¬è™«..."))
        crawler = CmaCatalogCrawler(url, self)

        try:
            video_list = crawler.crawl()
            if options['max_page'] > 0:
                video_list = video_list[:options['max_page'] * 20]

            self.stdout.write(self.style.SUCCESS(f"ğŸ” å…±å‘ç° {len(video_list)} ä¸ªè§†é¢‘é“¾æ¥"))

            for idx, item in enumerate(video_list, 1):
                if options['verbose']:
                    self.stdout.write(
                        self.style.HTTP_INFO(f"ğŸ”„ å¤„ç†è¿›åº¦: {idx}/{len(video_list)}") +
                        self.style.NOTICE(f" | å½“å‰URL: {item['url']}")
                    )

                try:
                    result = CmaVideoCrawler(
                        item['url'],
                        list_published_at=item['published_at']
                    ).run()

                    if result['status'] == 'success':
                        self.stdout.write(
                            self.style.SUCCESS(f"âœ… æˆåŠŸæŠ“å–: {result['title']} (ID: {result['video_id']})")
                        )
                    elif result['status'] == 'exists':
                        self.stdout.write(
                            self.style.WARNING(f"â© è·³è¿‡å·²å­˜åœ¨: {item['url']}")
                        )
                    else:
                        self.stdout.write(
                            self.style.ERROR(f"âŒ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                        )
                except Exception as e:
                    self.stdout.write(
                        self.style.ERROR(f"âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
                    )
                    logger.error(f"è§†é¢‘å¤„ç†å¤±è´¥: {item['url']} - {str(e)}")

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"ğŸ’¥ ç›®å½•å¤„ç†å¤±è´¥: {str(e)}"))
            logger.exception("ç›®å½•çˆ¬å–å¼‚å¸¸")

    def handle_single(self, url, options):
        """å¤„ç†å•ä¸ªè§†é¢‘é¡µ"""
        try:
            result = CmaVideoCrawler(url).run()
            if result['status'] == 'success':
                self.stdout.write(
                    self.style.SUCCESS(f"âœ… æˆåŠŸæŠ“å–è§†é¢‘: {result['title']} (ID: {result['video_id']})")
                )
            else:
                self.stdout.write(
                    self.style.ERROR(f"âŒ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                )
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"ğŸ’¥ å¤„ç†å¤±è´¥: {str(e)}"))

    def handle(self, *args, **options):
        if 'col/col982/index' in options['url']:
            self.handle_catalog(options['url'], options)
        else:
            self.handle_single(options['url'], options)